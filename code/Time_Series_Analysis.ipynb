{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Time Series Analysis"
      ],
      "metadata": {
        "id": "xbeKIraoxaFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time series analysis** is a statistical technique and a branch of data analysis that focuses on studying and modeling data points collected, recorded, or observed over a sequential and equally spaced time interval. Time series data represents **observations of a variable or phenomenon at different points in time**, and the goal of time series analysis is to extract meaningful insights, patterns, and predictions from this data."
      ],
      "metadata": {
        "id": "kKhr7PHQxiue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "diVU4zRyxz7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code begins by importing necessary Python libraries for data analysis, visualization, time series modeling, and performance evaluation. The libraries included are:\n",
        "\n",
        "\n",
        "1.   **Pandas** - Pandas is a powerful data manipulation and analysis library for Python. It provides data structures and functions for working with structured data, such as tables or spreadsheets.\n",
        "2.   **Numpy** - NumPy (Numerical Python) is a fundamental library for numerical and mathematical operations in Python. It provides support for arrays and matrices, essential for scientific and numerical computations.\n",
        "3.   **Matplotlib** - Matplotlib is a data visualization library for creating static, animated, or interactive plots and charts. It is widely used for creating graphical representations of data.\n",
        "4.   **Warnings** - The warnings module is a built-in Python library for controlling the issuance of warning messages in your code. It allows you to suppress or handle warnings, making your code cleaner and more robust.\n",
        "5.   **Statsmodels** - Statsmodels is a Python library for estimating and interpreting various statistical models. It is particularly useful for time series analysis, regression analysis, and hypothesis testing.\n",
        "6.   **Scikit-learn** - Scikit-Learn is a machine learning library that provides a wide range of machine learning algorithms and tools for data mining and data analysis. It is designed for easy integration into data analysis workflows.\n"
      ],
      "metadata": {
        "id": "mAUHgjjUx271"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from datetime import datetime, timedelta\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
      ],
      "metadata": {
        "id": "9Yo-9Qajg-eu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "8o59IAVQqVxf"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "Dwj6b9NhysPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is loaded from a CSV file (\"train.csv\") into a Pandas DataFrame. The initial exploration includes displaying the first few rows and summary statistics of the dataset.\n",
        "\n",
        "The code converts the \"Order Date\" column to datetime format, sorts the DataFrame by date, and then aggregates sales data into a time series using the groupby function."
      ],
      "metadata": {
        "id": "99Hku_nlywUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "print(df.head())\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "AeF2FhVEjxvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
        "\n",
        "df = df.sort_values('Order Date')\n",
        "sales_time_series = df.groupby('Order Date')['Sales'].sum()"
      ],
      "metadata": {
        "id": "hwo44wMtkq4_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "7atbPU61zCuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis (EDA)** is a systematic and critical process of analyzing, summarizing, and visualizing data to understand its key characteristics, uncover patterns, identify anomalies, and generate hypotheses. EDA is typically the initial phase of data analysis, and it plays a fundamental role in the data science and decision-making process. Advanced techniques that involved in EDA phase are:\n",
        "\n",
        "1.   **Rolling Statistics for Seasonality and Trends** - The **rolling mean** (also known as a moving average) is calculated to smooth out short-term fluctuations or noise in time series data. The **rolling standard deviation** measures the volatility or variability of data points within a moving window. It is used to identify periods of high or low volatility.\n",
        "\n",
        "2.   **Autocorrelation (ACF)** - Autocorrelation measures the correlation between a time series and its lagged values. It reveals how current data points depend on previous ones.\n",
        "3.   **Partial Autocorrelation (PACF)** - Partial autocorrelation, in contrast to ACF, measures the direct correlation between a time series and its lagged values while accounting for the influence of intermediate lags.\n",
        "\n"
      ],
      "metadata": {
        "id": "iUc9yNd4zh0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(sales_time_series)\n",
        "plt.title('Sales Over Time')\n",
        "plt.xlabel('Order Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q2azBxkNlCle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rolling Statistics for Seasonality and Trends\n",
        "rolling_mean = sales_time_series.rolling(window=7).mean()  # 7-day rolling mean for weekly seasonality\n",
        "rolling_std = sales_time_series.rolling(window=7).std()    # 7-day rolling standard deviation\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(sales_time_series, label='Original')\n",
        "plt.plot(rolling_mean, label='7-Day Rolling Mean')\n",
        "plt.plot(rolling_std, label='7-Day Rolling Std')\n",
        "plt.title('Rolling Mean & Standard Deviation')\n",
        "plt.xlabel('Order Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "Zf_uYUEVsLHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACF and PACF Plots for Autocorrelation\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(211)\n",
        "plot_acf(sales_time_series, lags=30, ax=plt.gca())\n",
        "plt.subplot(212)\n",
        "plot_pacf(sales_time_series, lags=30, ax=plt.gca())\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "g083LIvfspci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a date range for the next 7 days\n",
        "last_date = sales_time_series.index.max()\n",
        "forecast_start_date = last_date + timedelta(days=1)\n",
        "forecast_end_date = last_date + timedelta(days=7)\n",
        "date_range = pd.date_range(forecast_start_date, forecast_end_date)"
      ],
      "metadata": {
        "id": "MVqpEndYnnL1"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Holts-Winters Exponential Smoothing Forecasting"
      ],
      "metadata": {
        "id": "76JgwUns3JXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Holt-Winters Exponential Smoothing Forecasting** is a sophisticated time series forecasting technique that incorporates three primary components: level (or base), trend, and seasonality, to model and predict future values in time series data. It is a powerful and versatile method commonly used in business and economics for short to medium-term forecasting."
      ],
      "metadata": {
        "id": "Y68gzg8e3SVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Holt-Winters Exponential Smoothing model\n",
        "model = ExponentialSmoothing(sales_time_series, trend='add', seasonal='add', seasonal_periods=7)\n",
        "model_fit = model.fit(optimized=True)"
      ],
      "metadata": {
        "id": "by9y3UDPnu-L"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forecast sales for the next 7 days\n",
        "forecasted_sales = model_fit.forecast(steps=7)\n",
        "forecast_df = pd.DataFrame({'Date': date_range, 'Forecasted Sales': forecasted_sales})\n",
        "print(forecast_df)"
      ],
      "metadata": {
        "id": "ytoaRtiSnzRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validation Using TimeSeriesSplit"
      ],
      "metadata": {
        "id": "YOwX8oOy3pWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Validation with TimeSeriesSplit** is employed to assess the model's performance. The time series is divided into training and validation subsets to simulate forecasting and validation. For each fold, the Holt-Winters model is fit to the training data, and forecasts are generated for the validation set. Mean Absolute Error (MAE) and Mean Squared Error (MSE) are calculated for each fold."
      ],
      "metadata": {
        "id": "gcdTeHno3vX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "mae_scores = []\n",
        "mse_scores = []\n",
        "\n",
        "for train_index, test_index in tscv.split(sales_time_series):\n",
        "    train_data, validation_data = sales_time_series.iloc[train_index], sales_time_series.iloc[test_index]\n",
        "\n",
        "    # Fit the Holt-Winters Exponential Smoothing model on the training data\n",
        "    model = ExponentialSmoothing(train_data, trend='add', seasonal='add', seasonal_periods=7)\n",
        "    model_fit = model.fit(optimized=True)\n",
        "\n",
        "    # Forecast sales for the validation set (simulating the next 7 days)\n",
        "    forecasted_sales = model_fit.forecast(steps=len(validation_data))\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mae = mean_absolute_error(validation_data, forecasted_sales)\n",
        "    mse = mean_squared_error(validation_data, forecasted_sales)\n",
        "\n",
        "    mae_scores.append(mae)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Calculate the mean and standard deviation of MAE and MSE scores\n",
        "mean_mae = np.mean(mae_scores)\n",
        "mean_mse = np.mean(mse_scores)\n",
        "std_mae = np.std(mae_scores)\n",
        "std_mse = np.std(mse_scores)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mean_mae:.2f} ± {std_mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mean_mse:.2f} ± {std_mse:.2f}\")"
      ],
      "metadata": {
        "id": "Rw8A9CS3oIla",
        "outputId": "3116c6a4-0cf1-459a-bffc-d72ec0290923",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 1711.65 ± 87.53\n",
            "Mean Squared Error (MSE): 6385888.14 ± 1317647.57\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}